{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Aos5lXSuSG8nV7aRnXFiG7HimAh2hPUA","timestamp":1711906192216}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Mapping Challenge\n","**Treinamento de rede neural para localização de casas em imagens Satelitais**\n","\n","Como proposto na Prova técnica do FIESC-Senai, será treinado uma rede neural Fast R-CNN"],"metadata":{"id":"H5fqjmJyie9c"}},{"cell_type":"markdown","source":["# Inicialização do Google Drive\n","**Extração do Dataset do Zip**"],"metadata":{"id":"TlZ_QG8Ckjad"}},{"cell_type":"code","source":["# Montar o Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Caminho para o arquivo ZIP no Google Drive\n","zip_path = '/content/drive/My Drive/building.zip'\n","\n","# Diretório de destino para extrair os arquivos\n","extract_path = '/content/dataset'\n","\n","# Extrair o arquivo ZIP\n","import zipfile\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_path)\n","\n","# Verificar os arquivos extraídos\n","import os\n","os.listdir(extract_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oWo_9B_fkiCl","outputId":"338b4799-bdfb-4c8d-b5b9-d63c0317e527","executionInfo":{"status":"ok","timestamp":1711892610066,"user_tz":180,"elapsed":194094,"user":{"displayName":"ProvaFiescSenai2 Dataset","userId":"09201087514476298695"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"execute_result","data":{"text/plain":["['annotation.json', 'annotation-small.json', 'train']"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","source":["# Pré-Processamento do Dataset"],"metadata":{"id":"1sSZ5AB6cXp_"}},{"cell_type":"code","source":["import json\n","\n","# Carregar o arquivo annotations.json\n","annotations_file = '/content/drive/My Drive/annotation.json'\n","with open(annotations_file, 'r') as f:\n","    data = json.load(f)\n","\n","# Iterar sobre as informações das imagens\n","for img_info in data['images']:\n","    img_id = img_info['id']\n","    width = img_info['width']\n","    height = img_info['height']\n","\n","    # Verificar se a largura e altura são diferentes de 300\n","    if width != 300 or height != 300:\n","        img_path = os.path.join(data_dir, img_info['file_name'])\n","        print(f\"Image ID: {img_id}, Width: {width}, Height: {height}, Path: {img_path}\")\n"],"metadata":{"id":"yquCEYUlRdXw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Carregar o arquivo de anotações\n","annotations_file = '/content/drive/My Drive/annotation.json'\n","with open(annotations_file, 'r') as f:\n","    annotations = json.load(f)\n","\n","# Criar um conjunto de todos os IDs de imagem\n","image_ids = set(img_info['id'] for img_info in annotations['images'])\n","\n","# Criar um conjunto de todos os IDs de imagem anotados\n","annotated_image_ids = set(img_info['image_id'] for img_info in annotations['annotations'])\n","\n","# Encontrar os IDs de imagem sem anotações correspondentes\n","missing_annotations = image_ids - annotated_image_ids\n","\n","# Printar os IDs de imagem sem anotações correspondentes\n","if missing_annotations:\n","    print(\"IDs de imagem sem anotações correspondentes:\")\n","    for img_id in missing_annotations:\n","        print(img_id)\n","else:\n","    print(\"Todas as imagens têm anotações correspondentes.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bhooY06zYaee","executionInfo":{"status":"ok","timestamp":1711819371304,"user_tz":180,"elapsed":57696,"user":{"displayName":"ProvaFiesc Senai","userId":"11586760922597821349"}},"outputId":"8817f52d-3211-4eb7-86f6-977e8a1c7a71"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Todas as imagens têm anotações correspondentes.\n"]}]},{"cell_type":"code","source":["\n","# Caminho para o arquivo de anotação JSON original\n","annotation_file = '/content/drive/My Drive/annotation.json'\n","\n","# Caminho para o novo arquivo de anotação JSON sem as anotações inválidas\n","new_annotation_file = '/content/drive/My Drive/n-annotation.json'\n","\n","# Carregar o arquivo de anotação JSON\n","with open(annotation_file, \"r\") as f:\n","    data = json.load(f)\n","\n","# Inicializar lista para armazenar IDs de imagens com anotações válidas\n","valid_image_ids = []\n","\n","# Inicializar dicionário para armazenar anotações válidas\n","valid_annotations = {}\n","\n","# Percorrer todas as imagens no conjunto de dados\n","for img_id, img_info in data[\"images\"].items():\n","    # Obter IDs das anotações para a imagem atual\n","    ann_ids = data[\"annotations\"].get(img_id, [])\n","    # Verificar cada anotação para a imagem atual\n","    for ann_id in ann_ids:\n","        # Verificar se a anotação tem o campo 'bbox'\n","        if 'bbox' not in data[\"annotations\"][ann_id]:\n","            # Se não tiver, continuar para a próxima anotação\n","            continue\n","        # Obter largura e altura da caixa delimitadora\n","        x, y, w, h = data[\"annotations\"][ann_id]['bbox']\n","        # Verificar se a largura e a altura são positivas\n","        if w > 0 and h > 0:\n","            # Se forem, adicionar ID da imagem à lista de IDs de imagens com anotações válidas\n","            valid_image_ids.append(img_id)\n","            # Adicionar a anotação ao dicionário de anotações válidas\n","            valid_annotations[ann_id] = data[\"annotations\"][ann_id]\n","\n","# Atualizar o campo images no novo conjunto de dados com as imagens válidas\n","data[\"images\"] = {img_id: img_info for img_id, img_info in data[\"images\"].items() if img_id in valid_image_ids}\n","# Atualizar o campo annotations no novo conjunto de dados com as anotações válidas\n","data[\"annotations\"] = valid_annotations\n","\n","# Salvar as anotações atualizadas em um novo arquivo JSON\n","with open(new_annotation_file, \"w\") as f:\n","    json.dump(data, f)\n","\n","print(\"Anotações válidas foram salvas em\", new_annotation_file)"],"metadata":{"id":"UWZ9WhP1dmxC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inicialização do Dataset"],"metadata":{"id":"6_bkYgTQaIDi"}},{"cell_type":"code","source":["import os\n","import torch\n","import torch.utils.data\n","import torchvision\n","from PIL import Image\n","from pycocotools.coco import COCO\n","\n","class myOwnDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, annotation, transforms=None):\n","        self.root = root\n","        self.transforms = transforms\n","        self.coco = COCO(annotation)\n","        self.ids = list(sorted(self.coco.imgs.keys()))\n","\n","    def __getitem__(self, index):\n","        # Objeto da Biblioteca Coco\n","        coco = self.coco\n","        # Index das Imagens\n","        img_id = self.ids[index]\n","        #  Index das Anotações\n","        ann_ids = coco.getAnnIds(imgIds=img_id)\n","        coco_annotation = coco.loadAnns(ann_ids)\n","        path = coco.loadImgs(img_id)[0]['file_name']\n","        img = Image.open(os.path.join(self.root, path))\n","        # Número de Objetos anotados na imagem\n","        num_objs = len(coco_annotation)\n","\n","        # Bounding boxes das imagens\n","        # Formato biblioteca coco bbox = [xmin, ymin, width, height]\n","        # Convertendo para o formato pytorch = [xmin, ymin, xmax, ymax]\n","        boxes = []\n","        for i in range(num_objs):\n","            xmin = coco_annotation[i]['bbox'][0]\n","            ymin = coco_annotation[i]['bbox'][1]\n","            xmax = xmin + coco_annotation[i]['bbox'][2]\n","            ymax = ymin + coco_annotation[i]['bbox'][3]\n","            boxes.append([xmin, ymin, xmax, ymax])\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","\n","\n","        labels = torch.ones((num_objs,), dtype=torch.int64)\n","\n","        img_id = torch.tensor([img_id])\n","        # Size of bbox (Rectangular)\n","        areas = []\n","        for i in range(num_objs):\n","            areas.append(coco_annotation[i]['area'])\n","        areas = torch.as_tensor(areas, dtype=torch.float32)\n","\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","\n","        # Convertendo anotação para formato de dicionário\n","        my_annotation = {}\n","        my_annotation[\"boxes\"] = boxes\n","        my_annotation[\"labels\"] = labels\n","        my_annotation[\"image_id\"] = img_id\n","        my_annotation[\"area\"] = areas\n","        my_annotation[\"iscrowd\"] = iscrowd\n","\n","        if self.transforms is not None:\n","            img = self.transforms(img)\n","\n","        return img, my_annotation\n","\n","    def __len__(self):\n","        return len(self.ids)"],"metadata":{"id":"S2KHvFkUNen2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_transform():\n","    custom_transforms = []\n","    custom_transforms.append(torchvision.transforms.ToTensor())\n","    return torchvision.transforms.Compose(custom_transforms)"],"metadata":{"id":"9X8Py1eVNjBv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inicialização do DataLoader"],"metadata":{"id":"-qV7JdY_jhQk"}},{"cell_type":"code","source":["\n","train_data_dir = r\"/content/dataset/train/images\"\n","train_coco = r\"/content/drive/MyDrive/n-annotation.json\"\n","\n","\n","my_dataset = myOwnDataset(root=train_data_dir,\n","                          annotation=train_coco,\n","                          transforms=get_transform()\n","                          )\n","\n","#Adaptação do Dataset para rodar um conjunto menor de imagens para teste do código\n","image_indices = list(range(100))\n","my_dataset2 = data.Subset(my_dataset, image_indices)\n","\n","# collate_fn  custom  para o formato COCO\n","def collate_fn(batch):\n","    return tuple(zip(*batch))\n","\n","train_batch_size = 8\n","\n","\n","data_loader = torch.utils.data.DataLoader(my_dataset2,\n","                                          batch_size=train_batch_size,\n","                                          shuffle=True,\n","                                          num_workers=2,\n","                                          collate_fn=collate_fn)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a9CuSIOeNlct","executionInfo":{"status":"ok","timestamp":1711893849197,"user_tz":180,"elapsed":39565,"user":{"displayName":"ProvaFiescSenai2 Dataset","userId":"09201087514476298695"}},"outputId":"127d3828-3399-4fee-aed4-a41809b6664a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loading annotations into memory...\n","Done (t=34.36s)\n","creating index...\n","index created!\n"]}]},{"cell_type":"markdown","source":["# Treinamento com Early Stopping"],"metadata":{"id":"97qF9saXZjK8"}},{"cell_type":"code","source":["import torch\n","import torch.utils.data\n","import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","# Incialização do modelo Fast RCNN treinado do MS COCO\n","def get_model_instance_segmentation(num_classes):\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","    return model\n","\n","num_classes = 2\n","num_epochs = 10\n","patience = 10\n","best_loss = float('inf')\n","best_model_state = None\n","\n","\n","model = get_model_instance_segmentation(num_classes)\n","\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","model.to(device)\n","\n","\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for imgs, annotations in data_loader:\n","        imgs = list(img.to(device) for img in imgs)\n","        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n","        loss_dict = model(imgs, annotations)\n","        losses = sum(loss for loss in loss_dict.values())\n","\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","\n","        running_loss += losses.item()\n","\n","    epoch_loss = running_loss / len(data_loader)\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}')\n","\n","    if epoch_loss < best_loss:\n","        best_loss = epoch_loss\n","\n","        best_model_state = model.state_dict()\n","\n","        patience_counter = 0\n","    else:\n","\n","        patience_counter += 1\n","\n","    if patience_counter >= patience:\n","        print(f'Early stopping após {patience} épocas sem melhorias.')\n","        break\n","\n","torch.save(best_model_state, 'best_model.pt')"],"metadata":{"id":"dJK6HhShZnIF"},"execution_count":null,"outputs":[]}]}